# [GnB íŠ¸ë Œë“œ ì„¸ë¯¸ë‚˜] ê²Œì„ê³¼ Al

Created by: í˜„ì‹ ê¹€
Last edited by: í˜„ì‹ ê¹€
Tags: article/lecture
status: completed

## What happend in AI in 2022?

- [Jim Fan on Twitter: "TOP 10 AI spotlights, from a breathtaking 2022" / Twitter](https://twitter.com/DrJimFan/status/1607746957753057280)
    - Text â†’ Image
        - DALL-E 2: first large-scale Diffusion Model. OpenAI proprietary
        - Stable Diffusion: open-source. Stability AI + RunwayML. V.2.1 on single GPU
        - Imagen: Google proprietary
    - Text â†’ Text
        - ChatGPT: GPT-3.5 + RLHF
            
            [https://twitter.com/DrJimFan/status/1600884299435167745](https://twitter.com/DrJimFan/status/1600884299435167745)
            
    - Text â†’ Robot
        - Google RT-1
            
            [https://twitter.com/DrJimFan/status/1602776866380578816](https://twitter.com/DrJimFan/status/1602776866380578816)
            
    - Text â†’ Video
        - Meta Make-A-Video
            
            [https://twitter.com/deviparikh/status/1575506287009226752](https://twitter.com/deviparikh/status/1575506287009226752)
            
        - Google Imagen Video
        - Google Phenaki
    
    - AI playing games
        - MineDojo (NVidia): AI playing Minecraft (â€™22.11)
            
            [https://twitter.com/DrJimFan/status/1595459499732926464](https://twitter.com/DrJimFan/status/1595459499732926464)
            
        - [Learning to play Minecraft with Video PreTraining - OpenAI](https://openai.com/research/vpt) (â€™22.6)
        - CICERO playing Diplomacy (Meta) (â€™22.11)
            
            [https://twitter.com/MetaAI/status/1595075884502855680](https://twitter.com/MetaAI/status/1595075884502855680)
            
        - Agent to play Diplomacy (DeepMind) (â€™22.12)
            
            [https://twitter.com/DeepMind/status/1600160847518584832](https://twitter.com/DeepMind/status/1600160847518584832)
            
        - DeepMind DreamerV3 to collect diamonds in Minecraft (â€™23.1)
            
            [https://twitter.com/deepmind/status/1613159943040811010](https://twitter.com/deepmind/status/1613159943040811010)
            
        - Nexto RL agent (â€™23.1)
            
            [https://twitter.com/xiao_ted/status/1613657870918381568](https://twitter.com/xiao_ted/status/1613657870918381568)
            
    - Audio â†’ Text
        - [Whisper (OpenAI)](https://openai.com/blog/whisper/)
    - Text â†’ Audio
        - VALL-E (Microsoft) (â€™23.1)
            
            [https://twitter.com/DrJimFan/status/1611397525541617665](https://twitter.com/DrJimFan/status/1611397525541617665)
            
    - Bio
        - AlphaFold â€˜protein universeâ€™ (DeepMind)
    - Animation
        
        [https://twitter.com/angrypenguinPNG/status/1613594169334980608](https://twitter.com/angrypenguinPNG/status/1613594169334980608)
        
- What is happening in 2023?  Already. (as of â€˜23.3.15)
    - Search war: Bing+ChatGPT, Google Bard, â€¦ NeevaAI, You.com
    - Chat agents: ChatGPT, Bard, [Anthropic Claude](https://scale.com/blog/chatgpt-vs-claude), [DeepMind Sparrow](https://www.deepmind.com/blog/building-safer-dialogue-agents), LAION Open Assistant
    - Audio
        - [Jim Fan on Twitter: "FOUR audio models in the past week alone. If 2022 is the year of pixels for generative AI, then 2023 is the year of sound waves." / Twitter](https://twitter.com/DrJimFan/status/1621189554517729280)
        - MusicLM (Google) (text-to-audio), Music Caps (Google) (5.5k music-txt dataset), Sing Song (Google) (voice to instruments), Mousai (text-to-music), AudioLDM (text-to-audio), Make-an-audio (Meta) (image/video-to-audio)
    - BYOD â€œChatGPTâ€ (2.7)
        
        [Jim Fan on Twitter: "Microsoft will let companies create their own ChatGPT. â€œBYODâ€: Bring Your Own Data" / Twitter](https://twitter.com/DrJimFan/status/1623354315594432512)
        
        ![Untitled](%5BGnB%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%A6%E1%84%86%E1%85%B5%E1%84%82%E1%85%A1%5D%20%E1%84%80%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%80%E1%85%AA%20Al%20f921eefd9d6d482fa3a1bfbb0c18a929/Untitled.png)
        
    - Decentralized Training ê°œë…
        
        [https://twitter.com/DrJimFan/status/1629878724886949888](https://twitter.com/DrJimFan/status/1629878724886949888)
        
    - Robotics
        - [Google PaLM-E](https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html) (3.10)
            
            [Danny Driess on Twitter: "What happens when we train the largest vision-language model and add in robot experiences? PaLM-E, a 562-billion parameter, general-purpose, embodied visual-language generalist - across robotics, vision, and language" / Twitter](https://twitter.com/DannyDriess/status/1632904675124035585)
            
            [Pete Florence on Twitter: "PaLM-E can do things across robotics, vision, and languageâ€¦ letâ€™s look at a few capabilities in detail" / Twitter](https://twitter.com/peteflorence/status/1634256566907211776)
            
            - zero-shot visual chain-of-thoughts (CoT), Multimodal CoT, many-step zero-shot CoT, multimodal reasoning with CoT, multi-image reasoning
        
        [MimicPlay](https://twitter.com/chenwang_j/status/1628792565385564160)
        
    - Multi-modal LLM
        
        [Salesforce BLIP-2](https://twitter.com/lijunnan0409/status/1620259379223343107) (1.31)
        
        [MS Kosmos-1](https://twitter.com/omarsar0/status/1630392643602599937) (2.28)
        
        [Visual ChatGPT](https://twitter.com/_akhaliq/status/1633642479869198337) (3.9)
        
    - [ContronNet](https://github.com/lllyasviel/ControlNet), surpassing Stable Diffusion
    - New text-to-image models
        
        [poli on Twitter: "New text-to-image models that have been publicly announced to be cooking" / Twitter](https://twitter.com/multimodalart/status/1633072753732640768)
        
        - IF by DeepFloyd
        - StableDiffusion-XL by StabilityAI
        - Composer by Alibaba
        - DALLÂ·E 2 experimental by OpenAI
        - MidJourney v5
        - The-Model-After-SDXL by StabilityAI
        - [Muse](https://muse-model.github.io/)
        
    - [Meta LLaMA](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/) (2.24)
        
        [https://twitter.com/AiBreakfast/status/1629349156388245507](https://twitter.com/AiBreakfast/status/1629349156388245507)
        
        [https://twitter.com/diegofiori_/status/1630198791100923904](https://twitter.com/diegofiori_/status/1630198791100923904)
        
    - LLMâ€™s â€œStable Diffusion momentâ€
        
        [Meta's LLaMA Leaked - GPT Caliber Large Language Model Now In The Wild - Creative Tech Digest](https://creativetechnologydigest.substack.com/p/metas-llama-leaked-gpt-caliber-large) (3.7)
        
        - [llama.cpp](https://github.com/ggerganov/llama.cpp) â€œPersonal LLMâ€
            
            [Jason Phang on Twitter: "Code for fine-tuning LLaMA. One version using PEFT+8bit, and another using (simple) pipeline parallelism for full fine-tuning" / Twitter](https://twitter.com/zhansheng/status/1633622177336418304) (3.9)
            
            [Georgi Gerganov on Twitter: "I think I can make 4-bit LLaMA-65B inference run on a 64 GB M1 Pro. Speed should be somewhere around 2 tokens/sec" / Twitter](https://twitter.com/ggerganov/status/1632422491682484230) (3.6)
            
            [Georgi Gerganov on Twitter: "4-bit inference of LLaMA-7B using ggml: Pure C/C++, runs on the CPU at 20 tokens/sec (M1 Pro) Generated text looks coherent, but quickly degrades" / Twitter](https://twitter.com/ggerganov/status/1634282694208114690) (3.11)
            
            [Georgi Gerganov on Twitter: "Just added support for all LLaMA models. LLaMA-13B at ~10 tokens/s" / Twitter](https://twitter.com/ggerganov/status/1634488664150487041) (3.11)
            
            [Georgi Gerganov on Twitter: "Simultaneously running LLaMA-7B (left) + Whisper Small (right) on M1 Pro" / Twitter](https://twitter.com/ggerganov/status/1634320862722551809) (3.11)
            
            [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp | Simon Willison](https://til.simonwillison.net/llms/llama-7b-m2) (3.11)
            
            [Lawrence Chen on Twitter: "LLaMA 65B running on m1 max/64gb" / Twitter](https://twitter.com/lawrencecchen/status/1634507648824676353) (3.11)
            
            [Artem Andreenko on Twitter: "LLaMA 7B model on my 4GB RAM Raspberry Pi 4" / Twitter](https://twitter.com/miolini/status/1634982361757790209) (3.13)
            
            [Lewis on Twitter: "Node wrapper around llama.cpp for running LLaMA locally" / Twitter](https://twitter.com/ctjlewis/status/1635068768853585923) (3.13)
            
            [Jan Kaiser on Twitter: "a short comparison table of LLaMA_MPS versus @ggerganovâ€™s llama.cpp implementation" / Twitter](https://twitter.com/jankais3r/status/1634998316214915078) (3.13)
            
        - [Alpaca-7B](https://crfm.stanford.edu/2023/03/13/alpaca.html) â€œFine-tuned personal LLMâ€
            
            [anton on Twitter: "LLaMA has been fine-tuned by stanford, "We performed a blind pairwise comparison between text-davinci-003 and Alpaca 7B, and found that these two models have very similar performance: Alpaca wins 90 versus 89 comparisons against text-davinci-003"" / Twitter](https://twitter.com/abacaj/status/1635355642289618944) (3.14)
            
            [RaphaÃ«l MilliÃ¨re on Twitter: "The pace of development with LLaMA is vertiginous. If the finetuned 7B model is remotely in the same league as GPT-3.5 (text-davinci-003), can be reproduced for $100 and run on consumer laptops, this is really a turning point for LLMs (including implications for responsible use)." / Twitter](https://twitter.com/raphaelmilliere/status/1635373586818166786) (3.14)
            
        - â€œ20B LLM with RLHF on 24GB consumer GPUâ€
            
            [Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU - Hugging Face](https://huggingface.co/blog/trl-peft)
            
        - [OpenChatKit](https://www.together.xyz/blog/openchatkit) â€œPersonal ChatGPTâ€
            
            [Itamar Golan on Twitter: "OpenChatKit - The first open-source alternative to ChatGPT. 20B chat-GPT model, fine-tuned for chat using EleutherAI's GPT-NeoX-20B, with over 43 million instructions under the Apache-2.0 license" / Twitter](https://twitter.com/itakgol/status/1634590622286741504) (3.12)
            
        - Open-source instruction dataset
            
            [LAION OIG Dataset](https://laion.ai/blog/oig-dataset/)
            
        
        [Large language models are having their Stable Diffusion moment - Simon Willison](https://simonwillison.net/2023/Mar/11/llama/)
        
    - SOTA LLM ì„±ëŠ¥ ë¹„êµ
        
        ![Untitled](%5BGnB%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%A6%E1%84%86%E1%85%B5%E1%84%82%E1%85%A1%5D%20%E1%84%80%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%80%E1%85%AA%20Al%20f921eefd9d6d482fa3a1bfbb0c18a929/Untitled%201.png)
        

---

## 2023ë…„ í˜„ì¬ ì¼ì–´ë‚˜ê³  ìˆëŠ” ì¼ë“¤

## ì§€ê¸ˆ ë‹¹ì¥ ë¬´ì—‡ì´ ê°€ëŠ¥í•œê°€?

****CAN CHATGPT AND AI REALLY CREATE A GAME?****

- Create a basic 3D game prototype like Doom using GPT-4. Let GPT do all the maths, raycasting and hard work (â€™23.3.16)
    
    [https://twitter.com/javilopen/status/1636085116400451584](https://twitter.com/javilopen/status/1636085116400451584)
    

## AIëŠ” ê²Œì„ ì œì‘ ë¶„ì•¼ì—ì„œ ê°€ì¥ ë¨¼ì €â€¦

### AI-generated voice

- [Eleven Labs](https://beta.elevenlabs.io/)
    
    [https://twitter.com/CoffeeVectors/status/1620807387073642496](https://twitter.com/CoffeeVectors/status/1620807387073642496)
    

---

## ì•ìœ¼ë¡œ ì–´ë–¤ ì¼ë“¤ì´ ì¼ì–´ë‚ ê¹Œ

### 1. ê²Œì„ ì œì‘

- [The Generative AI Revolution in Games | a16z](https://a16z.com/2022/11/17/the-generative-ai-revolution-in-games/) (â€™22.11)
    - ìš”ì•½
        
        [https://twitter.com/gwertz/status/1593268718506692609](https://twitter.com/gwertz/status/1593268718506692609)
        
    
    **Predictions**:
    
    - **Generative AI for Games Market Map (updated â€˜23.3)**
        
        ![Untitled](%5BGnB%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%A6%E1%84%86%E1%85%B5%E1%84%82%E1%85%A1%5D%20%E1%84%80%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%80%E1%85%AA%20Al%20f921eefd9d6d482fa3a1bfbb0c18a929/Untitled%202.png)
        
        - 2D: Concept Art, production art
        - 3D: assets, textures, motion capture/filter from video, level design, world building
        - Audio: ìŒí–¥ íš¨ê³¼, ìŒì•…, ìŒì„±, ëŒ€í™”
        - NPC, ê²Œì„ ìºë¦­í„°

**íŠ¹íˆ AIGC ë¶„ì•¼ì—ì„œëŠ”â€¦**

- [The Generative AI Revolution will Enable Anyone to Create Games | a16z](https://a16z.com/2023/03/17/the-generative-ai-revolution/) (â€™23.3.17)
    
    From UGC games (Roblox, Minecraft) to AIGC (AI-generated contents)
    
    - AIGC ì ìš© 2ë‹¨ê³„:
        - AI-powered tools: UGC workflow (ìƒì‚°ì„±) ê°œì„ 
        - ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ (AI-native engine): to re-imagine the creation workflows from the ground-up
        
        [https://twitter.com/JoshLu/status/1636751574155526145](https://twitter.com/JoshLu/status/1636751574155526145)
        
    - AI-powered tooling
        - AI + ì¸ê°„ì˜ ê³µë™ ì œì‘:
            - co-authoring (ê²Œì„ ì•„ì´í…œ)
            - co-writing (ìŠ¤í† ë¦¬, ì„¸ê³„ê´€. [AI Dungeon](https://play.aidungeon.io/),Â [Electric Noir](https://www.electricnoirstudios.com/))
            - co-pilot (ì½”ë”©. auto-generating games (e.g. [Snake](https://twitter.com/ammaar/status/1635754631228952576?s=20), [Tetris](https://twitter.com/icreatelife/status/1642346286476144640)) on GPT-4)
                
                [https://www.youtube.com/watch?v=XMpguNJOs1U](https://www.youtube.com/watch?v=XMpguNJOs1U)
                
        - í”„ë¡¬í”„íŠ¸ ë§ˆì¼“: like Roblox Creator Marketplace, Unreal Asset Store, orÂ [PromptBase](https://www.promptbase.com/)
        - ìƒˆë¡œìš´ ê²Œì„ í”Œë ˆì´:
            - new mechanics or genres.
            - ì‹¤ì‹œê°„ custom ê²Œì„ ì•„ì´í…œ ìƒì„± ([Role](https://www.playrole.com/), [Riftweaver](https://www.riftweaver.com/))
        - Contents discovery:
            - ê²Œì„ ì•„ì´í…œì˜ semantic ê²€ìƒ‰ ([Loci.ai](https://www.loci.ai/))
        - í¬ë¦¬ì—ì´í„°ë“¤ì„ ìœ„í•œ ìœ ë£Œí™” ëª¨ë¸
        - UGC ëª¨ë‹ˆí„°ë§ & moderation: [GGWP](https://www.ggwp.com/)
    - ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ (AI-native engine)
        - Verticalë¡œ ì‹œì‘ (ì˜ˆ: íŠ¹ì • ì¥ë¥´ ê²Œì„ìš© íˆ´) â†’ (Minecraft ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ) í™•ì¥.ì„±ì¥
            - [Hidden Door](https://www.hiddendoor.co/) for storytelling games
            - [Roleverse](https://www.roleverse.com/) for tabletop RPG
            - [Regression Games](https://www.regression.gg/) for battlebot games)
        - ìƒˆë¡œìš´ ë°©ì‹ì˜ AI-enabled engine:
            - ì˜ˆ: ì‹¤ì‹œê°„ ìƒì„± (ë°°ê²½, ìºë¦­í„°, rendering)
            - ì˜ˆ: ë™ì˜ìƒ â†’ ê²Œì„ë‚´ ë°°ê²½ ì‹¤ì‹œê°„ ìƒì„±
            - [Luma Labs](https://lumalabs.ai/) (3D scanning and asset generation)
    

### 3. ìƒìƒ ì†ì˜ AI + ê²Œì„ í”Œë ˆì´

- AI + ê²Œì„ í”Œë ˆì´
    - ê° ê²Œì´ë¨¸ì— íŠ¹í™”, í•™ìŠµëœ NPC
    - ê²Œì„ì´ ëª¨ë“  ê²Œì´ë¨¸ ëŒ€ìƒ ê°œì¸í™”
    - ì‚¬ìš©ìë§ˆë‹¤ ê°œì¸í™”ëœ ê²Œì„ ì‹œë‚˜ë¦¬ì˜¤, ì•„ì´í…œ & í”Œë ˆì´ ì‹¤ì‹œê°„ ìƒì„±
        
        [In 20 years there will be no web pages. You do not believe me? - Latent Space](https://javilopen.substack.com/p/in-20-years-there-will-be-no-web)
        
    - â€¦.

---

## ì•ìœ¼ë¡œ ë¬´ì—‡ì´ ê°€ëŠ¥í•´ì§ˆê¹Œ? ìƒìƒì˜ ë‚˜ë˜ë¥¼ í¼ì³ ë³´ë©´â€¦

(ì´ì œ ìŠ¤ìŠ¤ë¡œì—ê²Œ ì§ˆë¬¸ì„ ë˜ì ¸ ë´…ì‹œë‹¤. ë¬´ì—‡ì´ ê°€ëŠ¥í•˜ê²Œ ë  ê²ƒì´ê³ , ë¬´ì—‡ì´ ê°€ëŠ¥í•˜ë©´ ì¢‹ì„ì§€, ë˜ â€¦)

### **â€œêµ¬ë£¨ì—ê²Œ ë¬»ëŠ”ë‹¤â€ - ì†¡ì¬ê²½ ëŒ€í‘œì™€ì˜ Fireside Chat**

- ë¨¼ì €, ì²˜ìŒì— ë°”ëŒì˜ë‚˜ë¼, ê·¸ë¦¬ê³  ë¦¬ë‹ˆì§€ë¥¼ ì²˜ìŒ ë§Œë“¤ ë•Œ, ì–´ë–¤ ìƒê°ì„ í•˜ë©´ì„œ ì–´ë–¤ ê¸°ëŒ€ë¥¼ í•˜ë©´ì„œ ì‹œì‘í•˜ì˜€ëŠ”ì§€? 
ê·¸ ì´ì „ì— ì–´ë–¤ í™˜ê²½ì—ì„œ ì–´ë–¤ ê²Œì„ì„ í”Œë ˆì´í•˜ë©´ì„œ ì–´ë–¤ ëŠë‚Œì„ ë°›ì•˜ê³  ë¬´ì—‡ì„ ë§Œë“¤ì–´ ë³´ê³  ì‹¶ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì´ì•¼ê¸°ë¶€í„° ì‹œì‘í•´ë„ ì¢‹ì„ ë“¯ í•©ë‹ˆë‹¤.
    
    â†’ ë‹¹ì‹œ í…ìŠ¤íŠ¸ ë¨¸ë“œ ì¥¬ë¼ê¸° ê³µì›ì´ ì›” 5ì²œë§Œì›ì˜ ë§¤ì¶œ â†’ ê·¸ê²ƒ ë³´ë‹¤ëŠ” ì˜ ë ê±°ë¼ ë¯¿ìŒ(ê·¸ë˜í”½ì´ ìˆìœ¼ë‹ˆê¹Œ)
    
    â†’ ë©€í‹°í”Œë ˆì´ì–´: í…ìŠ¤íŠ¸ ë¨¸ë“œ, ì¿¼í„°ë·° RPG (ìš¸í‹°ë§ˆ 6), RPG (ë„·í•µ)
    
    â†’ â€œë„·í•µì¸ë° ìš¸í‹°ë§ˆ 6 ê·¸ë˜í”½ì¸ ê²Œì„ì„ ë©€í‹° í”Œë ˆì´ì–´ë¡œ í•  ìˆ˜ ìˆìœ¼ë©´ ì •ë§ ì¬ë°Œì„í…ë°â€ê°€ ì‹œì‘ì 
    

- ë¦¬ë‹ˆì§€ë¥¼ ë°œí‘œí•˜ê³  ë§‰ìƒ ê·¸ ê²Œì„ì´ ë‹¹ì´ˆì— ìƒìƒí–ˆë˜ ê²ƒë³´ë‹¤ ë” ë¹ ë¥´ê²Œ, ë” í¬ê²Œ ì„±ê³µì„ ê±°ë‘ì—ˆë‹¤ê³  ë³´ëŠ”ë°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ê²ƒì„ ëŠê¼ˆê³ , ì–´ë–¤ ê²ƒì„ ìƒˆë¡œ ê¹¨ë‹³ì•˜ê³ , ì–´ë–¤ ê²ƒì€ â€˜ì´ë ‡ê²Œ í•˜ë©´ ë” ì¢‹ì•˜ì„ ê±¸â€™í•˜ëŠ” ìƒê°ì´ ë“¤ì—ˆëŠ”ì§€?
    
    â†’ ì ì ˆí•œ íƒ€ì´ë°ê³¼ ìš´ì´ ì¤‘ìš”í•˜ë‹¤(ì´ˆê³ ì† ì¸í„°ë„·, IMF, PCë°©, ë””ì•„ë¸”ë¡œ, ìŠ¤íƒ€í¬ë˜í”„íŠ¸, â€¦)
    
    â†’ ê²Œì„(ë˜ëŠ” ì»¨í…ì¸ )ë¥¼ ë°›ì³ì¤„ ê¸°ìˆ ë ¥ì´ ì¤‘ìš”í•˜ë‹¤(Windows Server, IO Completion Port, DB Server, ë™ì ‘ 1000ëª… ëŒíŒŒ)
    

- ì´ë²ˆ AI ê¸°ìˆ ì´ ë“±ì¥í•˜ê³  ëŒ€ì¤‘ì´ ì´ì— í™˜í˜¸í•˜ëŠ” ëª¨ìŠµì„ ë³´ë©´ì„œ, 90ë…„ëŒ€ ê²½í—˜í•˜ì˜€ë˜ ê²ƒê³¼ ì–´ë–¤ ìœ ì‚¬í•¨ì„ ëŠë¼ëŠ”ì§€? ê·¸ë ‡ë‹¤ë©´, ê·¸ë–„ì™€ ë‹¤ë¥¸ ì ì€ ì–´ë–¤ ê²ƒì„ ëŠë¼ëŠ”ì§€?
ê¸°ì¡´ì— ì—†ë˜ ìƒˆë¡œìš´ ë¬´ì—‡ì„ (ì¥ë¥´ë¥¼) ì°½ì¡°í•´ë‚´ê³  ì´ ê²ƒì´ ëŒ€ì¤‘ì—ì„œ ë³´í¸í™”ë˜ëŠ” ê³¼ì •ì„ ê²½í—˜í•œ â€˜êµ¬ë£¨â€™ ì…ì¥ì—ì„œ ì–´ë–¤ ë¹„ìŠ·í•œ íŒ¨í„´ì„ ëŠë‚„ ìˆ˜ ìˆëŠ”ì§€?
    
    â†’ ì¸í„°ë„·ì´ ë³´ê¸‰ë˜ë˜ 90ë…„ëŒ€ ë§ê³¼ ë¹„ìŠ·
    
    â†’ 90ë…„ëŒ€ ì´ˆì—ëŠ” ê·¹íˆ ì†Œìˆ˜(ì¼ë¶€ í•™êµ, ì—°êµ¬ì†Œ, ê¸°ì—…)ì—ì„œë§Œ ì¸í„°ë„· ì‚¬ìš©
    
    â†’ 90ë…„ëŒ€ ë§ì— ì´ˆê³ ì† ì¸í„°ë„· ë³´ê¸‰(Windows 95, Netscape, Internet Explorer)
    
    â†’ ê²Œì„ì´ ë³´ê¸‰ì— ì¤‘ìš”í•œ ì—­í• ì„ í•¨(Battle.net, ë¦¬ë‹ˆì§€, â€¦)
    
    â†’ AIë„ ì¼ë¶€ ì‹¤í—˜ì‹¤ê³¼ íšŒì‚¬ì—ì„œë§Œ ìˆë‹¤ê°€ ChatGPTë¡œ í­ë°œì ìœ¼ë¡œ ëŒ€ì¤‘ì  ê´€ì‹¬ì„ ì–»ê²Œ ë¨
    
    â†’ ChatGPTê°€ ì¸í„°ë„·, ì›¹ë¸Œë¼ìš°ì € ê°™ì€ í”Œë«í¼ ì—­í• ì„ í•˜ê²Œ ë˜ê³ , ê·¸ ìœ„ì— ì¸í„°ë„· ë³´ê¸‰ ì‹œì ˆ ê²Œì„ ì—­í• ê³¼ ë¹„ìŠ·í•œ ì–´ë–¤ ì–´í”Œë¦¬ì¼€ì´ì…˜ì´ ë‚˜ì˜¬ê¹Œ?
    

- ë§Œì¼ ê·¸ë ‡ë‹¤ë©´ ì•ìœ¼ë¡œ ê²Œì„ ë¶„ì•¼ì— AI ê¸°ìˆ ë¡œ ì¸í•œ ì–´ë–¤ ë³€í™”ê°€ ìƒê²¨ë‚ ê¹Œ ì— ëŒ€í•˜ì—¬ ì–´ë–¤ ìƒê°ì„ ê°€ì§€ê³  ìˆëŠ”ì§€? 
(ì§€ê¸ˆ ë‹¹ì¥ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ê¸°ìˆ ì ì¸ ê¸°ëŠ¥ ë“±ì— êµ¬ì•  ë°›ì§€ ì•Šê³ , ì•ìœ¼ë¡œ 10ë…„ì˜ ê¸°ê°„ì„ ë‘ê³  ì–´ë–¤ ë³€í™”ê°€ ì˜¬ê±°ë¼ ì˜ˆìƒí•˜ëŠ”ì§€)
    
    

- ë‹¤ë¥´ê²Œ ì´ì•¼ê¸°í•˜ë©´ ë¬´ì–¸ê°€ ìƒˆë¡œìš´ ì¥ë¥´ë¥¼ ë§Œë“¤ ê¸°íšŒê°€ ì¸ìƒì—ì„œ í•œë²ˆ ë” ì£¼ì–´ì¡Œë‹¤ê³  ë³¼ ìˆ˜ë„ ìˆëŠ”ë°, ì–´ë–»ê²Œ ìƒê°í•˜ê³  ì–´ë–»ê²Œ í•˜ë ¤ê³  í•˜ëŠ”ì§€ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ ë“£ê³  ì‹¶ìŒ.
    
    

- ì´ë¥¼ ìœ„í•˜ì—¬ ì§€ê¸ˆ ì–´ë–¤ ì‹¤í—˜ ë“¤ì„ í•´ ë³´ê³  ìˆëŠ”ì§€? ìˆë‹¤ë©´ ì–´ë–¤ ê²ƒë“¤ì¸ì§€ ì†Œê°œí•´ ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?
    
    

### ì²­ì¤‘ ì§ˆë¬¸ë“¤

(ê¹€ê¸°ì •)

- ì €ë„ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ì„ ê²Œì„ê³¼ ê²°í•©í•œë‹¤ë©´ ê°ê°ì˜ ìœ ì €ê°€ ê²Œì„ ì†ì—ì„œ ê°œì¸í™”ëœ ê²½í—˜ì„ ì¦ê¸¸ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ê²Œì„ì„ ì§€ì†ì ìœ¼ë¡œ ë°œì „ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ìœ ì €ê°€ ê²Œì„ ì•ˆì—ì„œ ì–´ë–¤ ê²½í—˜ì„ í•˜ê³  ìˆëŠ”ì§€ ë¶„ì„í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ ê°œì¸í™”ëœ ê²½í—˜ì„ ì¼ì¼ì´ ë¶„ì„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í• ê¹Œìš”? ë¶ˆê°€ëŠ¥í•˜ë‹¤ë©´, ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ê°œì¸í™”ëœ ê²½í—˜ì„ ë¶„ì„í•˜ì—¬ ê²Œì„ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œìš”?
    
    â‡’ ìœ ì €ì˜ ë¡œê·¸ë¥¼ ìŒ“ê³  ì´ê±¸ í•™ìŠµ(í˜¹ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§)í•˜ë©´ ì¶©ë¶„íˆ ê°€ëŠ¥í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
    ê²Œì„ì´ ì œê³µí•˜ê³  ìˆëŠ” ì»¨í…ì¸  ì¤‘ì— ìœ ì €ê°€ ì•ˆí•´ë³¸ ê²ƒì´ ìˆë‹¤ë©´, ì ì ˆí•œ í€˜ìŠ¤íŠ¸ ë“±ì„ AIê°€ ë§ì¶¤ ìƒì„±í•´ì„œ ìœ ì €ì—ê²Œ ì œê³µí•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ.
    â‡’ ìœ ì €ì˜ ë¡œê·¸ ë¶„ì„ ë“±ì„ í†µí•´ì„œ ìœ ì €ê°€ ë§¤ë ¥ì„ ëŠë‚„ ë§Œí•œ NPCë¥¼ AIê°€ ìƒì„±í•´ì„œ ì—­í• ê·¹ì„ í•˜ê²Œ
    

- í˜„ì¬ ChatGPTì™€ ê°™ì€ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ì„ APIë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ ê²Œì„ê³¼ ê°™ì´ ì¸ê³µì§€ëŠ¥ê³¼ ìœ ì €ì˜ ìƒí˜¸ì‘ìš©ì´ ë§¤ìš° ë§ì€ ê²½ìš°, ì½œë‹¹ ë¹„ìš©ì„ ë‚´ëŠ” API ë¹„ìš©ì´ ê°ë‹¹ ê°€ëŠ¥í• ê¹Œìš”? ê°ë‹¹í•˜ì§€ ëª»í•œë‹¤ë©´ ì–´ë–¤ ì˜µì…˜ì´ ìˆì„ê¹Œìš”?
    
    â‡’ ëª¨ë°”ì¼ ê²Œì„ì— ìœ ì €ë“¤ì´ ì‚¬ìš©í•˜ëŠ” ì•¡ìˆ˜ë¥¼ ë³´ë©´ ê°€ëŠ¥í•  ê²ƒ ê°™ì€ë° ğŸ˜Š
    ì•ìœ¼ë¡œ API ë¹„ìš©ì´ ë‚®ì•„ì§ˆ ê²ƒì´ë¼ê³  ê¸°ëŒ€í•©ë‹ˆë‹¤. ì¸í„°ë„· ë³´ê¸‰ì— ë”°ë¼ ê´€ë ¨ í”Œë«í¼ ë¹„ìš©ì´ ë§¤ìš° ë‚®ì•„ì§„ ê²ƒì²˜ëŸ¼.
    

- ê²Œì„ ì—…ê³„ì—ì„œëŠ” ìœ ì € ë°ì´í„° ë³´ì•ˆ ë¬¸ì œê°€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë“¤ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì¬ ë‚˜ì˜¤ëŠ” ëŒ€ë¶€ë¶„ì˜ ìµœì²¨ë‹¨ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” APIë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„°ë¥¼ ì™¸ë¶€ë¡œ ë…¸ì¶œì‹œí‚¤ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë§Œì•½ ìì²´ ëª¨ë¸ì„ ê°€ì§€ê³  ìˆì§€ ì•Šë‹¤ë©´, ì´ëŸ¬í•œ ë¬¸ì œê°€ ìˆìŒì—ë„ íƒ€ ê¸°ì—…ì˜ ì¸ê³µì§€ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ì„œê¹Œì§€ ì¸ê³µì§€ëŠ¥ì„ ê²Œì„ì— ì ‘ëª©ì‹œí‚¤ë ¤ê³  í• ê¹Œìš”?
    
    â‡’ ë‹¤ë¥¸ ìœ ì €ê°€ ê³µê°œì ìœ¼ë¡œ ì¸ì§€í•  ìˆ˜ ìˆëŠ” í–‰ë™ê³¼ ëŒ€í™”ë§Œì„ AIì—ê²Œ ë³´ë‚´ëŠ” ê²ƒìœ¼ë¡œ ê°€ëŠ¥í•˜ì§€ ì•Šì„ê¹Œ. 
    AIê°€ ì €ë ´í•´ì ¸ì„œ ê²Œì„ íšŒì‚¬ë“¤ì´ ë‚´ë¶€ì—ì„œ ë™ì‘ì‹œí‚¤ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§€ì§€ ì•Šì„ê¹Œ.
    

(ë°•ë³‘ì€)

- Generative AIê°€ ê²Œì„ì‚°ì—…ì—ì„œ ì ìš©ë  ìˆ˜ ìˆëŠ” ì‚¬ë¡€ë¡œ NPC ì ìš©ì´ë‚˜ ì œì‘ë‹¨ê³„ì—ì„œì˜ ë¦¬ì†ŒìŠ¤ ì ˆê° ë“±ì´ ìˆì„í…ë°ìš”, ê±°ëŒ€ ê²Œì„ì‚¬ì—ê²Œ ìœ ë¦¬í• ê¹Œìš”? í˜¹ì€ ìƒˆë¡œ ì‹œì‘í•˜ëŠ” ê¸°ì—…ë“¤ì´ ë¹ ë¥´ê²Œ ì—¬ëŸ¬ ê²Œì„ì œì‘ê³¼ ì¶œì‹œë¥¼ ì‹¤í—˜í•´ë³¼ìˆ˜ ìˆì–´ì„œ ë” ìœ ë¦¬í• ê¹Œìš”?
    
    â‡’ ë‘êµ°ë° ë‹¤ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒ.
    â†’ ì•„ì´ë””ì–´ë§Œ ìˆë‹¤ë©´ ê·¸ë¦¼ì„ ëª»ê·¸ë¦¬ê±°ë‚˜ í”„ë¡œê·¸ë¨ì„ ëª»ì§œë„ ê²Œì„ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë  ë“¯
    â†’ ìˆ˜ë§ì€ ì¸ë””ê²Œì„ í­ë°œ
    â†’ í° ê¸°ì—…ì—ì„œë„ AI ì‚¬ìš©ì„ í†µí•´ ìƒì‚°ì„±ì´ í–¥ìƒ ë  ê²ƒì´ë¯€ë¡œ ì œì‘ê¸°ê°„ì´ ë‹¨ì¶•ë˜ê±°ë‚˜, ì ì€ ì¸ì›ìœ¼ë¡œ ê°™ì€ ê²Œì„ì„ ë§Œë“¤ìˆ˜ ìˆê²Œ ë˜ê±°ë‚˜, ê°™ì€ ì¸ì›ìœ¼ë¡œ ë” ë§ì€ ê²Œì„ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë  ë“¯.
    

(ê¹€ìƒê· )

- ë°˜ëŒ€ë¡œ ê²Œì„ì‚°ì—…ì´ AIì˜ ì ìš© ë° í™œìš©ì— í° ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œê°€ ìˆì„ì§€ë„ ìƒê°í•´ ë³´ê²Œ ë˜ëŠ”ë°ìš”. ê²Œì„ì—ì„œ ì¼ì–´ë‚˜ëŠ” ìˆ˜ë§ì€ ì•„ì´í…œê³¼ ë°°ê²½ ë“¤ì˜ ë³€í™” ì—…ë°ì´íŠ¸, ìœ ì €ì— ëŒ€í•œ Customizing ë“± ì–´ë–¤ ë¶€ë¶„ì´ AIì˜ í•„ìš”ë¥¼ ë” í•˜ê²Œ ë˜ëŠ” ê²ƒì¼ê¹Œìš”?
    
    â‡’ í•„ìš”í•œ ê±´ ì•„ë‹ˆì§€ë§Œ, AIì˜ ë„ì›€ìœ¼ë¡œ ìƒì‚°ì„± í–¥ìƒìœ¼ë¡œ ê³¼ê±°ì—ëŠ” ë¶ˆê°€ëŠ¥í–ˆì§€ë§Œ ê°€ëŠ¥í•´ì§€ëŠ” ì¼ë“¤ì´ ìˆì„ ê±° ê°™ìŠµë‹ˆë‹¤. 
    
    ì˜ˆë¥¼ ë“¤ì–´ ê°œì¸í™”ëœ NPC AIê°€ ë„ìš°ë¯¸ë¡œ ë”°ë¼ ë‹¤ë‹Œë‹¤ê±°ë‚˜. ë§¤ë²ˆ í•  ë•Œë§ˆë‹¤ ë‹¬ë¼ì§€ëŠ” ìŠ¤í† ë¦¬ë¥¼ ê°€ì§„ ê²Œì„ì´ë¼ë˜ê°€.
    

(ê³½í˜„ì¼)

- ë³¸ê²©ì ìœ¼ë¡œ ì´ˆê¸° ìƒì„± AI í‚¬ëŸ¬ì•±/ê²Œì„ì€ ì‹œê°„ ì‹¸ì›€ì— ì ‘ì–´ë“  ëŠë‚Œì¸ë° êµ¬ë£¨ë‹˜ë“¤ì´ë¼ë©´ ì–´ë–¤ ê²Œì„ í˜¹ì€ ì¥ë¥´ì— ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì ‘ëª©ì‹œì¼œë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
    
    â‡’ ë§¤ìš° ë»”í•œ ê²ƒì€ ì‚¬ëŒì²˜ëŸ¼ ë§í•˜ëŠ” NPC
    â‡’ ë§¤ë²ˆ ë‹¬ë¼ì§€ëŠ” ìŠ¤í† ë¦¬, í€˜ìŠ¤íŠ¸
    

- í˜„ì¬ëŠ” AI ê¸°ìˆ ì´ ì™„ì „í•˜ì§€ ì•Šë‹¤ë³´ë‹ˆ ë§Œì•½ ë‹¹ì¥ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•œë‹¤ë©´ ì–´ëŠ ê°•ì ì„ ê°•ì¡°í•˜ê³  ì–´ëŠ ì•½ì ì„ ìµœëŒ€í•œ ìˆ¨ê²¨ì•¼ í•  ê²ƒ ê°™ì€ì§€ë„ ê¶ê¸ˆí•©ë‹ˆë‹¤.
    
    â‡’ ì§€ê¸ˆ ë‹¹ì¥ì´ë¼ë©´ ê²Œì„ì˜ ì¤‘ìš” ë©”ì¹´ë‹ˆì¦˜ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ëŒ€í™” ê°™ì€ë° ë°”ë¡œ ì ìš© ê°€ëŠ¥í•  ë“¯
    â‡’ ë°¸ëŸ¬ìŠ¤/ë³´ìƒ ê°™ì€ ì¤‘ìš” ë©”ì¹´ë‹ˆì¦˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê³³ì— ë„ì…í•˜ëŠ” ê²ƒì€ ì¡°ì‹¬ 
    

(Kenneth Hong)

- ê²Œì„ ìŠ¤í† ë¦¬ë¼ì¸ ì œì‘ë‹¨ê³„ì—ì„œ GPTì ìš©ë²”ìœ„ê°€ ë‹¤ì–‘í•˜ê³  íš¨ê³¼ì ì´ê² ì§€ë§Œ ì¸ê°„ì˜ ì°½ì˜ì  ìŠ¤í† ë¦¬ì™€ì˜ ì°¨ë³„ì„±ì„ ìœ ì§€í•˜ê¸°ìœ„í•œ ì‘ê°€, ì¸ë ¥  ë§¤ë‹ˆì§€ë¨¼íŠ¸ê°€ ê¸°ì¡´ê³¼ ë‹¬ë¦¬ì§€ëŠ”ë¶€ë¶„ì„ ì•Œê³ ì‹¶ìŠµë‹ˆë‹¤.
    
    â‡’ â€œì¤‘ìš” ìŠ¤í† ë¦¬ â†’ ì¸ê°„ ê¸°íšì, ë‚´ìš©ì„ ì±„ìš°ê¸° ìœ„í•œ ë‹¨ìˆœ ìŠ¤í† ë¦¬ â†’ AIâ€ ê°™ì´ ì—­í•  ë¶„ë‹´í•  ìˆ˜ ìˆì„ ë“¯ 
    í˜¹ì€ ì¸ê°„ ê¸°íšìê°€ ì¼ì£¼ì¼ì— 1ê°œ ìŠ¤í† ë¦¬ë¥¼ ì“¸ ìˆ˜ ìˆì—ˆë‹¤ë©´, ì´ì œëŠ” AIì˜ ë„ì›€ì„ ë°›ì•„ í•˜ë£¨ì— 1ê°œ ì“¸ ìˆ˜ ìˆì„ ë“¯
    

(ë¥˜ì¸íƒœ)

- ê²Œì„ ë¶„ì•¼ì—ì„œ AI ë¥¼ ì ìš©í•œ ì‚¬ë¡€ì¤‘ ê°€ì¥ ì„íŒ©íŠ¸ê°€ ìˆì—ˆë˜ ì¼€ì´ìŠ¤ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?
    
    â‡’ ì§€ê¸ˆ ë‚˜ì˜¤ëŠ” ChatGPT ìˆ˜ì¤€ì˜ AIë¥¼ ë§í•˜ëŠ” ê²ƒì´ë¼ë©´ ì•„ì§ì€ ì—†ëŠ” ë“¯
    â‡’ í´ë˜ì‹í•œ AI (íŠ¸ë¦¬ íƒìƒ‰, A* ì•Œê³ ë¦¬ì¦˜, Behavior Tree, â€¦)ëŠ” ëª¨ë“  ê²Œì„ì´ ë‹¤ ì‚¬ìš©ì¤‘
    

(ì´ì„±ê·œ)

- í˜„ì¬ ê²Œì„ì—…ê³„ì—ì„œ ì ìš©ëœ ì¸ê³µì§€ëŠ¥ ë¶€í„° ì§šì–´ì£¼ì„¸ìš”.
    
    â‡’ ëŒ€ê·œëª¨ NNì„ ì‚¬ìš©í•œ ê²Œì„ì€ ì•„ì§ ì—†ëŠ” ë“¯
    â‡’ í´ë˜ì‹í•œ AIëŠ” ëª¨ë“  ê²Œì„ ì‚¬ìš©ì¤‘ (ê¸¸ì°¾ê¸° â‡’ A*, BT, â€¦)
    â‡’ ì»´í“¨í„°ê°€ í•˜ê²Œ ë˜ë©´ ì¸ê³µì§€ëŠ¥ì´ë¼ê³  í•˜ì§€ ì•ŠëŠ”ë‹¤ ğŸ˜ƒÂ  (ë¬¸ì ì¸ì‹, í•„ê¸°ì²´ ì¸ì‹, ìŒì„± ì¸ì‹, ë²ˆì—­, â€¦)
    

- ì˜¤í”ˆì›”ë“œë¥˜ì˜ ê²Œì„ì—ì„œ NPC  í–‰ë™ ë“¤ì€ ëŒ€ë¶€ë¶„ ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ì—ì„œ ë°œì „ë  ìˆ˜ ìˆì„ì§€ ë“± ì‹¤ì œ ê²Œì„í”Œë ˆì´ì™€ ì—°ê´€ëœ ë¶€ë¶„ì´ ì œì¼ ê¶ê¸ˆí•©ë‹ˆë‹¤.
    
    â‡’ ì¼ë‹¨ ëŒ€í™” ë¶€ë¶„ì€ ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•  ë“¯
    â‡’ ìœ ì €ë¥¼ ë”°ë¼ ë‹¤ë‹ˆë©´ì„œ ë„ì™€ì£¼ëŠ” NPCë„ ì‰½ê²Œ ê°€ëŠ¥í•  ë“¯
    â‡’ 
    
    ![Untitled](%5BGnB%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%A6%E1%84%86%E1%85%B5%E1%84%82%E1%85%A1%5D%20%E1%84%80%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%80%E1%85%AA%20Al%20f921eefd9d6d482fa3a1bfbb0c18a929/Untitled%203.png)
    

- What is happening in 2023?  Already. (as of â€˜23.3.15)
    - Search war: Bing+ChatGPT, Google Bard, â€¦ NeevaAI, You.com
    - Chat agents: ChatGPT, Bard, [Anthropic Claude](https://scale.com/blog/chatgpt-vs-claude), [DeepMind Sparrow](https://www.deepmind.com/blog/building-safer-dialogue-agents), LAION Open Assistant
    - Audio
        - [Jim Fan on Twitter: "FOUR audio models in the past week alone. If 2022 is the year of pixels for generative AI, then 2023 is the year of sound waves." / Twitter](https://twitter.com/DrJimFan/status/1621189554517729280)
        - MusicLM (Google) (text-to-audio), Music Caps (Google) (5.5k music-txt dataset), Sing Song (Google) (voice to instruments), Mousai (text-to-music), AudioLDM (text-to-audio), Make-an-audio (Meta) (image/video-to-audio)
    - BYOD â€œChatGPTâ€ (2.7)
        
        [Jim Fan on Twitter: "Microsoft will let companies create their own ChatGPT. â€œBYODâ€: Bring Your Own Data" / Twitter](https://twitter.com/DrJimFan/status/1623354315594432512)
        
        ![Untitled](%5BGnB%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%A6%E1%84%86%E1%85%B5%E1%84%82%E1%85%A1%5D%20%E1%84%80%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%80%E1%85%AA%20Al%20f921eefd9d6d482fa3a1bfbb0c18a929/Untitled.png)
        
    - Decentralized Training ê°œë…
        
        [https://twitter.com/DrJimFan/status/1629878724886949888](https://twitter.com/DrJimFan/status/1629878724886949888)
        
    - 3D
        - Objaverse: open dataset of 1 million text-paired 3D objects (3.23)
            
            [https://twitter.com/mattdeitke/status/1638608472525897728](https://twitter.com/mattdeitke/status/1638608472525897728)
            
        - Zero-1-to-3: One Image to 3D Object (3.21)
            
            [https://twitter.com/_akhaliq/status/1637989614349606913](https://twitter.com/_akhaliq/status/1637989614349606913)
            
        - AI tool to create text-to-space (3.14)
            
            [https://twitter.com/therundownai/status/1635299221870919681](https://twitter.com/therundownai/status/1635299221870919681)
            
        - Text2Tex: Text-driven Texture Synthesis (3.22)
            
            [https://twitter.com/_akhaliq/status/1638338037636538368](https://twitter.com/_akhaliq/status/1638338037636538368)
            
        - â€œGPT-4 for Blenderâ€ (3.26)
            
            [https://twitter.com/rowancheung/status/1639702313186230272](https://twitter.com/rowancheung/status/1639702313186230272)
            
    - Robotics
        - [Google PaLM-E](https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html) (3.10)
            
            [Danny Driess on Twitter: "What happens when we train the largest vision-language model and add in robot experiences? PaLM-E, a 562-billion parameter, general-purpose, embodied visual-language generalist - across robotics, vision, and language" / Twitter](https://twitter.com/DannyDriess/status/1632904675124035585)
            
            [Pete Florence on Twitter: "PaLM-E can do things across robotics, vision, and languageâ€¦ letâ€™s look at a few capabilities in detail" / Twitter](https://twitter.com/peteflorence/status/1634256566907211776)
            
            - zero-shot visual chain-of-thoughts (CoT), Multimodal CoT, many-step zero-shot CoT, multimodal reasoning with CoT, multi-image reasoning
        
        [MimicPlay](https://twitter.com/chenwang_j/status/1628792565385564160)
        
    - Multi-modal LLM
        
        [Salesforce BLIP-2](https://twitter.com/lijunnan0409/status/1620259379223343107) (1.31)
        
        [MS Kosmos-1](https://twitter.com/omarsar0/status/1630392643602599937) (2.28)
        
        [Visual ChatGPT](https://twitter.com/_akhaliq/status/1633642479869198337) (3.9)
        
    - [ContronNet](https://github.com/lllyasviel/ControlNet), surpassing Stable Diffusion
    - New text-to-image models
        
        [poli on Twitter: "New text-to-image models that have been publicly announced to be cooking" / Twitter](https://twitter.com/multimodalart/status/1633072753732640768)
        
        - IF by DeepFloyd
        - StableDiffusion-XL by StabilityAI
        - Composer by Alibaba
        - DALLÂ·E 2 experimental by OpenAI
        - MidJourney v5
        - The-Model-After-SDXL by StabilityAI
        - [Muse](https://muse-model.github.io/)
        
    - [Meta LLaMA](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/) (2.24)
        
        [https://twitter.com/AiBreakfast/status/1629349156388245507](https://twitter.com/AiBreakfast/status/1629349156388245507)
        
        [https://twitter.com/diegofiori_/status/1630198791100923904](https://twitter.com/diegofiori_/status/1630198791100923904)
        
    - LLMâ€™s â€œStable Diffusion momentâ€
        
        [Meta's LLaMA Leaked - GPT Caliber Large Language Model Now In The Wild - Creative Tech Digest](https://creativetechnologydigest.substack.com/p/metas-llama-leaked-gpt-caliber-large) (3.7)
        
        - [llama.cpp](https://github.com/ggerganov/llama.cpp) â€œPersonal LLMâ€
            
            [Jason Phang on Twitter: "Code for fine-tuning LLaMA. One version using PEFT+8bit, and another using (simple) pipeline parallelism for full fine-tuning" / Twitter](https://twitter.com/zhansheng/status/1633622177336418304) (3.9)
            
            [Georgi Gerganov on Twitter: "I think I can make 4-bit LLaMA-65B inference run on a 64 GB M1 Pro. Speed should be somewhere around 2 tokens/sec" / Twitter](https://twitter.com/ggerganov/status/1632422491682484230) (3.6)
            
            [Georgi Gerganov on Twitter: "4-bit inference of LLaMA-7B using ggml: Pure C/C++, runs on the CPU at 20 tokens/sec (M1 Pro) Generated text looks coherent, but quickly degrades" / Twitter](https://twitter.com/ggerganov/status/1634282694208114690) (3.11)
            
            [Georgi Gerganov on Twitter: "Just added support for all LLaMA models. LLaMA-13B at ~10 tokens/s" / Twitter](https://twitter.com/ggerganov/status/1634488664150487041) (3.11)
            
            [Georgi Gerganov on Twitter: "Simultaneously running LLaMA-7B (left) + Whisper Small (right) on M1 Pro" / Twitter](https://twitter.com/ggerganov/status/1634320862722551809) (3.11)
            
            [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp | Simon Willison](https://til.simonwillison.net/llms/llama-7b-m2) (3.11)
            
            [Lawrence Chen on Twitter: "LLaMA 65B running on m1 max/64gb" / Twitter](https://twitter.com/lawrencecchen/status/1634507648824676353) (3.11)
            
            [Artem Andreenko on Twitter: "LLaMA 7B model on my 4GB RAM Raspberry Pi 4" / Twitter](https://twitter.com/miolini/status/1634982361757790209) (3.13)
            
            [Lewis on Twitter: "Node wrapper around llama.cpp for running LLaMA locally" / Twitter](https://twitter.com/ctjlewis/status/1635068768853585923) (3.13)
            
            [Jan Kaiser on Twitter: "a short comparison table of LLaMA_MPS versus @ggerganovâ€™s llama.cpp implementation" / Twitter](https://twitter.com/jankais3r/status/1634998316214915078) (3.13)
            
        - [Alpaca-7B](https://crfm.stanford.edu/2023/03/13/alpaca.html) â€œFine-tuned personal LLMâ€
            
            [anton on Twitter: "LLaMA has been fine-tuned by stanford, "We performed a blind pairwise comparison between text-davinci-003 and Alpaca 7B, and found that these two models have very similar performance: Alpaca wins 90 versus 89 comparisons against text-davinci-003"" / Twitter](https://twitter.com/abacaj/status/1635355642289618944) (3.14)
            
            [RaphaÃ«l MilliÃ¨re on Twitter: "The pace of development with LLaMA is vertiginous. If the finetuned 7B model is remotely in the same league as GPT-3.5 (text-davinci-003), can be reproduced for $100 and run on consumer laptops, this is really a turning point for LLMs (including implications for responsible use)." / Twitter](https://twitter.com/raphaelmilliere/status/1635373586818166786) (3.14)
            
        - â€œ20B LLM with RLHF on 24GB consumer GPUâ€
            
            [Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU - Hugging Face](https://huggingface.co/blog/trl-peft)
            
        - [OpenChatKit](https://www.together.xyz/blog/openchatkit) â€œPersonal ChatGPTâ€
            
            [Itamar Golan on Twitter: "OpenChatKit - The first open-source alternative to ChatGPT. 20B chat-GPT model, fine-tuned for chat using EleutherAI's GPT-NeoX-20B, with over 43 million instructions under the Apache-2.0 license" / Twitter](https://twitter.com/itakgol/status/1634590622286741504) (3.12)
            
        - Open-source instruction dataset
            
            [LAION OIG Dataset](https://laion.ai/blog/oig-dataset/)
            
        
        [Large language models are having their Stable Diffusion moment - Simon Willison](https://simonwillison.net/2023/Mar/11/llama/)
        
    - SOTA LLM ì„±ëŠ¥ ë¹„êµ
        
        ![Untitled](%5BGnB%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%A6%E1%84%86%E1%85%B5%E1%84%82%E1%85%A1%5D%20%E1%84%80%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%80%E1%85%AA%20Al%20f921eefd9d6d482fa3a1bfbb0c18a929/Untitled%201.png)