# LORA

Created by: 김현식
Last edited by: 현식 김
Tags: knowledge base
URL: https://replicate.com/blog/lora-faster-fine-tuning-of-stable-diffusion
status: completed

**LoRA - faster way to fine-tune Stable Diffusion**

LoRA stands for Low-Rank Adaptation, a mathematical technique to reduce the number of parameters that are trained

LoRA has a few differences from DreamBooth that make it especially appealing as an alternative:

- **Faster training**: Training a new concept with LoRA takes just a few minutes.
- **Smaller outputs**: Trained LoRA outputs are much smaller than DreamBooth outputs. This makes them easier to share, store, and re-use.
- **Multiple concepts**: You can combine multiple trained concepts in a single image. (This feature is still experimental, but we're working on improving it. 🧪)
- **Faster image generation**: When you train your own DreamBooth model on Replicate, the model only stays warm when you're actively using it. With LoRA, you're not running your own model, but rather running the one [cloneofsimo/lora](https://replicate.com/cloneofsimo/lora) model, which is always on and ready to serve predictions.
- **Better at styles, worse at faces.** Based on our experimentation, LoRA seems to do a better job at styles than DreamBooth, but faces aren't as good. They are stuck in uncanny valley, rather than looking precisely like the person. Your results might be better than ours though, so let us know how you get on.